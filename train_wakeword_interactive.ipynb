{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c55f7ceb",
   "metadata": {},
   "source": [
    "# Custom Wake Word Training\n",
    "\n",
    "This notebook guides you through training your own custom wake word model using openWakeWord.\n",
    "\n",
    "## What you'll learn:\n",
    "- ✅ How to prepare audio data\n",
    "- ✅ Extract audio features\n",
    "- ✅ Train a neural network model\n",
    "- ✅ Export and test your model\n",
    "\n",
    "## Requirements:\n",
    "- Positive examples: Audio clips of your wake phrase (100+ recommended)\n",
    "- Negative examples: Other speech, music, noise (1000+ recommended)\n",
    "- All audio should be 16kHz, mono, 16-bit WAV files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca045b0",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install and import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a0c708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements (uncomment if needed)\n",
    "# !pip install openwakeword torch scipy matplotlib datasets tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219f5f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import scipy.io.wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import openwakeword\n",
    "import openwakeword.data\n",
    "import openwakeword.utils\n",
    "import openwakeword.metrics\n",
    "\n",
    "print(\"✓ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43895553",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set your wake phrase and data directories here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a0d96f",
   "metadata": {},
   "source": [
    "## Step 0: Data Preparation (IMPORTANT!)\n",
    "\n",
    "Before training, you need audio data. There are three options:\n",
    "\n",
    "### **Option 1: Use Sample Datasets (Recommended for Testing)**\n",
    "Download pre-prepared sample datasets to quickly test the training pipeline.\n",
    "\n",
    "### **Option 2: Download Full Datasets**\n",
    "Get larger datasets for better model performance.\n",
    "\n",
    "### **Option 3: Use Your Own Audio**\n",
    "Record or collect your own audio files (skip to Configuration section).\n",
    "\n",
    "**Choose one option below and run the corresponding cells.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11db7b74",
   "metadata": {},
   "source": [
    "### Option 1: Download Sample Datasets (Quick Start)\n",
    "\n",
    "Run these cells to download small sample datasets for quick testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f31ca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory structure\n",
    "!mkdir -p training_data/positive\n",
    "!mkdir -p training_data/negative/speech\n",
    "!mkdir -p training_data/negative/music\n",
    "!mkdir -p training_data/negative/noise\n",
    "\n",
    "print(\"✓ Directory structure created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6ae776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample negative data: Speech from Common Voice\n",
    "# This will download ~5000 clips of English speech (about 500MB)\n",
    "\n",
    "import datasets\n",
    "\n",
    "print(\"Downloading Common Voice 11 test split...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "cv_11 = datasets.load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split=\"test\", streaming=True)\n",
    "cv_11 = cv_11.cast_column(\"audio\", datasets.Audio(sampling_rate=16000, mono=True))\n",
    "cv_11 = iter(cv_11)\n",
    "\n",
    "# Convert and save clips (first 5000 for sample)\n",
    "limit = 5000\n",
    "print(f\"Processing {limit} audio clips...\")\n",
    "\n",
    "for i in tqdm(range(limit)):\n",
    "    try:\n",
    "        example = next(cv_11)\n",
    "        output = os.path.join(\"training_data/negative/speech\", example[\"path\"].replace(\"/\", \"_\")[0:-4] + \".wav\")\n",
    "        os.makedirs(os.path.dirname(output), exist_ok=True)\n",
    "        \n",
    "        # Convert to 16-bit PCM format\n",
    "        wav_data = (example[\"audio\"][\"array\"] * 32767).astype(np.int16)\n",
    "        scipy.io.wavfile.write(output, 16000, wav_data)\n",
    "    except StopIteration:\n",
    "        print(f\"Reached end of dataset at {i} clips\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing clip {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"✓ Downloaded {limit} speech clips to training_data/negative/speech/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fce4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample music and noise datasets\n",
    "# These are pre-prepared sample datasets from openWakeWord\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# Download FMA music samples\n",
    "print(\"Downloading FMA music samples...\")\n",
    "music_url = \"https://f002.backblazeb2.com/file/openwakeword-resources/data/fma_sample.zip\"\n",
    "music_zip = \"fma_sample.zip\"\n",
    "\n",
    "try:\n",
    "    urllib.request.urlretrieve(music_url, music_zip)\n",
    "    with zipfile.ZipFile(music_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"training_data/negative/music/\")\n",
    "    os.remove(music_zip)\n",
    "    print(f\"✓ Downloaded music samples to training_data/negative/music/\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Failed to download music samples: {e}\")\n",
    "\n",
    "# Download FSD50k noise samples\n",
    "print(\"\\nDownloading FSD50k noise samples...\")\n",
    "noise_url = \"https://f002.backblazeb2.com/file/openwakeword-resources/data/fsd50k_sample.zip\"\n",
    "noise_zip = \"fsd50k_sample.zip\"\n",
    "\n",
    "try:\n",
    "    urllib.request.urlretrieve(noise_url, noise_zip)\n",
    "    with zipfile.ZipFile(noise_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"training_data/negative/noise/\")\n",
    "    os.remove(noise_zip)\n",
    "    print(f\"✓ Downloaded noise samples to training_data/negative/noise/\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Failed to download noise samples: {e}\")\n",
    "\n",
    "print(\"\\n✅ Sample dataset download complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c133fae5",
   "metadata": {},
   "source": [
    "### Option 2: Generate Positive Examples with TTS\n",
    "\n",
    "Since you need positive examples of your wake phrase, let's generate them using Text-to-Speech!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6b6772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Simple TTS with pyttsx3 (easiest, but lower quality)\n",
    "# Uncomment and run if you want quick synthetic data\n",
    "\n",
    "# !pip install pyttsx3\n",
    "\n",
    "# import pyttsx3\n",
    "# \n",
    "# engine = pyttsx3.init()\n",
    "# voices = engine.getProperty('voices')\n",
    "# \n",
    "# wake_phrase = \"hey jarvis\"  # Change this!\n",
    "# \n",
    "# print(f\"Generating {len(voices) * 10} clips with different voices...\")\n",
    "# \n",
    "# for i, voice in enumerate(voices):\n",
    "#     engine.setProperty('voice', voice.id)\n",
    "#     for j in range(10):  # 10 variations per voice\n",
    "#         output_file = f\"training_data/positive/tts_voice_{i}_clip_{j}.wav\"\n",
    "#         engine.save_to_file(wake_phrase, output_file)\n",
    "#     engine.runAndWait()\n",
    "# \n",
    "# print(f\"✓ Generated positive examples in training_data/positive/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4dfa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Record your own voice (recommended!)\n",
    "# Instructions:\n",
    "# 1. Use any recording app (Audacity, QuickTime, etc.)\n",
    "# 2. Record yourself saying the wake phrase 50-100 times\n",
    "# 3. Vary your tone, speed, and environment\n",
    "# 4. Save as WAV files: training_data/positive/recording_001.wav, etc.\n",
    "# 5. Ensure 16kHz, mono, 16-bit format\n",
    "\n",
    "# To convert existing recordings to correct format:\n",
    "# !ffmpeg -i input.wav -ar 16000 -ac 1 -sample_fmt s16 output.wav\n",
    "\n",
    "print(\"⚠️  IMPORTANT: You need positive examples of YOUR wake phrase!\")\n",
    "print(\"\")\n",
    "print(\"Quick option: Download a sample dataset for 'turn on the office lights'\")\n",
    "print(\"Run this cell to download:\")\n",
    "\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "positive_url = \"https://f002.backblazeb2.com/file/openwakeword-resources/data/turn_on_the_office_lights.tar.gz\"\n",
    "positive_tar = \"turn_on_the_office_lights.tar.gz\"\n",
    "\n",
    "try:\n",
    "    print(\"Downloading sample wake phrase clips...\")\n",
    "    urllib.request.urlretrieve(positive_url, positive_tar)\n",
    "    \n",
    "    with tarfile.open(positive_tar, 'r:gz') as tar:\n",
    "        tar.extractall(\"training_data/positive/\")\n",
    "    \n",
    "    os.remove(positive_tar)\n",
    "    print(\"✓ Downloaded ~3400 positive examples to training_data/positive/\")\n",
    "    print(\"  Wake phrase: 'turn on the office lights'\")\n",
    "    print(\"  Remember to update WAKE_PHRASE in Configuration cell!\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Download failed: {e}\")\n",
    "    print(\"  Please add your own recordings to training_data/positive/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edc6aa7",
   "metadata": {},
   "source": [
    "### Verify Your Dataset\n",
    "\n",
    "Run this to check what you have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2931af54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what data you have\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def count_wav_files(directory):\n",
    "    \"\"\"Count WAV files in a directory\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        return 0\n",
    "    return len(list(Path(directory).rglob(\"*.wav\")))\n",
    "\n",
    "positive_count = count_wav_files(\"training_data/positive\")\n",
    "speech_count = count_wav_files(\"training_data/negative/speech\")\n",
    "music_count = count_wav_files(\"training_data/negative/music\")\n",
    "noise_count = count_wav_files(\"training_data/negative/noise\")\n",
    "total_negative = speech_count + music_count + noise_count\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Positive examples (wake phrase):  {positive_count:,}\")\n",
    "print(f\"\\nNegative examples:\")\n",
    "print(f\"  Speech:  {speech_count:,}\")\n",
    "print(f\"  Music:   {music_count:,}\")\n",
    "print(f\"  Noise:   {noise_count:,}\")\n",
    "print(f\"  Total:   {total_negative:,}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if you have enough data\n",
    "print(\"\\nData Quality Check:\")\n",
    "if positive_count < 50:\n",
    "    print(\"❌ WARNING: Very few positive examples!\")\n",
    "    print(\"   Recommendation: Need at least 50-100, preferably 500+\")\n",
    "elif positive_count < 500:\n",
    "    print(\"⚠️  NOTICE: Limited positive examples\")\n",
    "    print(\"   Recommendation: 500+ examples for better results\")\n",
    "else:\n",
    "    print(\"✅ Good amount of positive examples\")\n",
    "\n",
    "if total_negative < 500:\n",
    "    print(\"❌ WARNING: Very few negative examples!\")\n",
    "    print(\"   Recommendation: Need at least 500-1000, preferably 5000+\")\n",
    "elif total_negative < 5000:\n",
    "    print(\"⚠️  NOTICE: Limited negative examples\")\n",
    "    print(\"   Recommendation: 5000+ examples for better results\")\n",
    "else:\n",
    "    print(\"✅ Good amount of negative examples\")\n",
    "\n",
    "print(\"\\nYou can proceed to Configuration and training below!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69de3e3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ✅ Data Preparation Complete!\n",
    "\n",
    "If you ran the cells above, you now have:\n",
    "- ✅ Directory structure created\n",
    "- ✅ Negative examples downloaded (speech, music, noise)\n",
    "- ✅ Option to generate or download positive examples\n",
    "- ✅ Dataset verification\n",
    "\n",
    "**Next:** Continue to the Configuration section below to set your wake phrase and start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b3cd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "WAKE_PHRASE = \"hey jarvis\"  # Change this to your wake phrase\n",
    "POSITIVE_DIR = \"training_data/positive\"  # Directory with positive examples\n",
    "NEGATIVE_DIRS = [  # Directories with negative examples\n",
    "    \"training_data/negative/speech\",\n",
    "    \"training_data/negative/music\",\n",
    "    \"training_data/negative/noise\"\n",
    "]\n",
    "OUTPUT_DIR = Path(\"training_output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Training parameters\n",
    "N_EPOCHS = 10\n",
    "BATCH_SIZE = 512\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "print(f\"Wake phrase: {WAKE_PHRASE}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa38fea",
   "metadata": {},
   "source": [
    "## Step 1: Data Collection\n",
    "\n",
    "Let's load and validate your audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b62c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get positive examples\n",
    "positive_dir = Path(POSITIVE_DIR)\n",
    "positive_clips_raw = list(positive_dir.glob(\"*.wav\"))\n",
    "print(f\"Found {len(positive_clips_raw)} positive examples\")\n",
    "\n",
    "# Get negative examples\n",
    "negative_clips_raw = []\n",
    "for neg_dir in NEGATIVE_DIRS:\n",
    "    neg_path = Path(neg_dir)\n",
    "    if neg_path.exists():\n",
    "        clips = list(neg_path.glob(\"*.wav\"))\n",
    "        negative_clips_raw.extend(clips)\n",
    "        print(f\"Found {len(clips)} negative examples in {neg_dir}\")\n",
    "    else:\n",
    "        print(f\"⚠️  Directory not found: {neg_dir}\")\n",
    "\n",
    "print(f\"\\nTotal: {len(positive_clips_raw)} positive, {len(negative_clips_raw)} negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a67c7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter clips by duration (2-8 seconds)\n",
    "print(\"Filtering clips by duration (2-8 seconds)...\")\n",
    "\n",
    "positive_clips, positive_durations = openwakeword.data.filter_audio_paths(\n",
    "    [str(p) for p in positive_clips_raw],\n",
    "    min_duration=2.0,\n",
    "    max_duration=8.0\n",
    ")\n",
    "\n",
    "negative_clips, negative_durations = openwakeword.data.filter_audio_paths(\n",
    "    [str(p) for p in negative_clips_raw],\n",
    "    min_duration=2.0,\n",
    "    max_duration=8.0\n",
    ")\n",
    "\n",
    "print(f\"\\nAfter filtering: {len(positive_clips)} positive, {len(negative_clips)} negative\")\n",
    "\n",
    "# Check if we have enough data\n",
    "if len(positive_clips) < 10:\n",
    "    print(\"\\n⚠️  WARNING: Very few positive examples! You need at least 50-100 for good results.\")\n",
    "if len(negative_clips) < 100:\n",
    "    print(\"\\n⚠️  WARNING: Few negative examples! You need at least 1000+ for good results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6193c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataset statistics\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Duration histogram\n",
    "axes[0].hist(positive_durations, bins=20, alpha=0.5, label='Positive')\n",
    "axes[0].hist(negative_durations, bins=20, alpha=0.5, label='Negative')\n",
    "axes[0].set_xlabel('Duration (seconds)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Audio Duration Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# Dataset size comparison\n",
    "axes[1].bar(['Positive', 'Negative'], [len(positive_clips), len(negative_clips)])\n",
    "axes[1].set_ylabel('Number of Clips')\n",
    "axes[1].set_title('Dataset Size')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa81ae4",
   "metadata": {},
   "source": [
    "## Step 2: Feature Extraction\n",
    "\n",
    "Convert audio to openWakeWord embeddings (28 timesteps × 96 features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7010a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature extractor\n",
    "print(\"Initializing audio feature extractor...\")\n",
    "feature_extractor = openwakeword.utils.AudioFeatures()\n",
    "print(\"✓ Feature extractor ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67d3839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio_clips, cache_file=None):\n",
    "    \"\"\"Extract features from audio clips with optional caching\"\"\"\n",
    "    \n",
    "    # Check for cached features\n",
    "    if cache_file and cache_file.exists():\n",
    "        print(f\"Loading cached features from {cache_file}\")\n",
    "        return np.load(cache_file)\n",
    "    \n",
    "    # Extract features\n",
    "    all_features = []\n",
    "    \n",
    "    for clip_path in tqdm(audio_clips, desc=\"Extracting features\"):\n",
    "        try:\n",
    "            # Read audio\n",
    "            rate, audio = scipy.io.wavfile.read(clip_path)\n",
    "            \n",
    "            # Verify sample rate\n",
    "            if rate != 16000:\n",
    "                print(f\"⚠️  Skipping {clip_path}: incorrect sample rate {rate}Hz (need 16kHz)\")\n",
    "                continue\n",
    "            \n",
    "            # Extract features\n",
    "            features = feature_extractor.get_features(audio)\n",
    "            all_features.append(features)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Error processing {clip_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Stack into array\n",
    "    features_array = np.vstack(all_features)\n",
    "    \n",
    "    # Cache if requested\n",
    "    if cache_file:\n",
    "        np.save(cache_file, features_array)\n",
    "        print(f\"✓ Saved features to {cache_file}\")\n",
    "    \n",
    "    return features_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef11ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract positive features\n",
    "model_name = WAKE_PHRASE.replace(' ', '_')\n",
    "positive_features = extract_features(\n",
    "    positive_clips,\n",
    "    cache_file=OUTPUT_DIR / f\"{model_name}_positive_features.npy\"\n",
    ")\n",
    "\n",
    "print(f\"Positive features shape: {positive_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbbfd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract negative features\n",
    "negative_features = extract_features(\n",
    "    negative_clips,\n",
    "    cache_file=OUTPUT_DIR / \"negative_features.npy\"\n",
    ")\n",
    "\n",
    "print(f\"Negative features shape: {negative_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e828418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample features\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Positive example\n",
    "axes[0].imshow(positive_features[0].T, aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[0].set_xlabel('Time Steps')\n",
    "axes[0].set_ylabel('Feature Dimension')\n",
    "axes[0].set_title('Positive Example Features')\n",
    "axes[0].colorbar()\n",
    "\n",
    "# Negative example\n",
    "axes[1].imshow(negative_features[0].T, aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[1].set_xlabel('Time Steps')\n",
    "axes[1].set_ylabel('Feature Dimension')\n",
    "axes[1].set_title('Negative Example Features')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3ff8e1",
   "metadata": {},
   "source": [
    "## Step 3: Model Training\n",
    "\n",
    "Train a neural network to classify wake word vs non-wake word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2356ff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "X = np.vstack((negative_features, positive_features))\n",
    "y = np.array([0] * len(negative_features) + [1] * len(positive_features)).astype(np.float32)[..., None]\n",
    "\n",
    "print(f\"Training data shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"Positive examples: {sum(y == 1)[0]}\")\n",
    "print(f\"Negative examples: {sum(y == 0)[0]}\")\n",
    "print(f\"Positive ratio: {sum(y == 1)[0] / len(y) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1335c5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch dataloader\n",
    "training_data = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(torch.from_numpy(X), torch.from_numpy(y)),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"✓ Created dataloader with batch size {BATCH_SIZE}\")\n",
    "print(f\"  Number of batches: {len(training_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc52075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "layer_dim = 32\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(X.shape[1] * X.shape[2], layer_dim),  # timesteps * features -> hidden\n",
    "    nn.LayerNorm(layer_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(layer_dim, layer_dim),\n",
    "    nn.LayerNorm(layer_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(layer_dim, 1),  # hidden -> output (0 or 1)\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "print(\"Model architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631a9c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "loss_function = torch.nn.functional.binary_cross_entropy\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"✓ Optimizer: Adam with learning rate {LEARNING_RATE}\")\n",
    "print(f\"✓ Loss function: Binary Cross Entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c074fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(f\"\\nTraining for {N_EPOCHS} epochs...\\n\")\n",
    "\n",
    "history = collections.defaultdict(list)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    epoch_losses = []\n",
    "    epoch_recalls = []\n",
    "    \n",
    "    progress_bar = tqdm(training_data, desc=f\"Epoch {epoch+1}/{N_EPOCHS}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        x, y_batch = batch[0], batch[1]\n",
    "        \n",
    "        # Weight classes: 10x penalty for false positives\n",
    "        # This helps reduce false activations\n",
    "        weights = torch.ones(y_batch.shape[0])\n",
    "        weights[y_batch.flatten() == 1] = 0.1\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_function(predictions, y_batch, weights[..., None])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        epoch_losses.append(float(loss.detach().numpy()))\n",
    "        \n",
    "        tp = sum(predictions.flatten()[y_batch.flatten() == 1] >= 0.5)\n",
    "        fn = sum(predictions.flatten()[y_batch.flatten() == 1] < 0.5)\n",
    "        recall = float(tp / (tp + fn).detach().numpy())\n",
    "        epoch_recalls.append(recall)\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'recall': f'{recall:.4f}'\n",
    "        })\n",
    "    \n",
    "    # Log epoch summary\n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    avg_recall = np.mean(epoch_recalls)\n",
    "    history['loss'].extend(epoch_losses)\n",
    "    history['recall'].extend(epoch_recalls)\n",
    "    history['epoch_loss'].append(avg_loss)\n",
    "    history['epoch_recall'].append(avg_recall)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{N_EPOCHS} Summary - Loss: {avg_loss:.4f}, Recall: {avg_recall:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb3912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training metrics\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Loss over time\n",
    "axes[0].plot(history['epoch_loss'])\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Recall over time\n",
    "axes[1].plot(history['epoch_recall'])\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Recall')\n",
    "axes[1].set_title('Training Recall')\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal training metrics:\")\n",
    "print(f\"  Loss: {history['epoch_loss'][-1]:.4f}\")\n",
    "print(f\"  Recall: {history['epoch_recall'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7791d4",
   "metadata": {},
   "source": [
    "## Step 4: Model Export\n",
    "\n",
    "Export the trained model to ONNX format for use with openWakeWord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1d9810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX\n",
    "onnx_path = OUTPUT_DIR / f\"{model_name}.onnx\"\n",
    "\n",
    "print(f\"Exporting model to {onnx_path}...\")\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    args=torch.zeros((1, 28, 96)),  # Input shape: (batch, timesteps, features)\n",
    "    f=str(onnx_path),\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    ")\n",
    "\n",
    "print(f\"✓ Model exported to: {onnx_path}\")\n",
    "print(f\"  File size: {onnx_path.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdd8e69",
   "metadata": {},
   "source": [
    "## Step 5: Model Testing\n",
    "\n",
    "Load and test the exported model with openWakeWord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46938653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with openWakeWord\n",
    "print(f\"Loading model with openWakeWord...\")\n",
    "\n",
    "oww_model = openwakeword.Model(\n",
    "    wakeword_model_paths=[str(onnx_path)],\n",
    "    enable_speex_noise_suppression=True,\n",
    "    vad_threshold=0.5\n",
    ")\n",
    "\n",
    "print(f\"✓ Model loaded successfully!\")\n",
    "print(f\"  Model name: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a10b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on positive examples\n",
    "print(\"\\nTesting on positive examples...\\n\")\n",
    "\n",
    "for i, clip_path in enumerate(positive_clips[:5]):\n",
    "    scores = oww_model.predict_clip(str(clip_path))\n",
    "    max_score = max(s[model_name] for s in scores)\n",
    "    print(f\"{i+1}. {Path(clip_path).name}\")\n",
    "    print(f\"   Max score: {max_score:.3f} {'✓ DETECTED' if max_score > 0.5 else '✗ MISSED'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89c9513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on negative examples\n",
    "print(\"\\nTesting on negative examples...\\n\")\n",
    "\n",
    "for i, clip_path in enumerate(negative_clips[:5]):\n",
    "    scores = oww_model.predict_clip(str(clip_path))\n",
    "    max_score = max(s[model_name] for s in scores)\n",
    "    print(f\"{i+1}. {Path(clip_path).name}\")\n",
    "    print(f\"   Max score: {max_score:.3f} {'✓ CORRECT' if max_score < 0.5 else '✗ FALSE POSITIVE'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6ccf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction on a sample clip\n",
    "sample_clip = positive_clips[0]\n",
    "scores = oww_model.predict_clip(str(sample_clip))\n",
    "score_values = [s[model_name] for s in scores]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(score_values)\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', label='Threshold (0.5)')\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('Detection Score')\n",
    "plt.title(f'Detection Scores on Positive Example: {Path(sample_clip).name}')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Max score: {max(score_values):.3f}\")\n",
    "print(f\"Mean score: {np.mean(score_values):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2011d5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Your custom wake word model is now trained and ready to use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d2cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nWake phrase: '{WAKE_PHRASE}'\")\n",
    "print(f\"Model file: {onnx_path}\")\n",
    "print(f\"Model size: {onnx_path.stat().st_size / 1024:.1f} KB\")\n",
    "print(f\"\\nTraining data:\")\n",
    "print(f\"  Positive examples: {len(positive_clips)}\")\n",
    "print(f\"  Negative examples: {len(negative_clips)}\")\n",
    "print(f\"\\nFinal metrics:\")\n",
    "print(f\"  Loss: {history['epoch_loss'][-1]:.4f}\")\n",
    "print(f\"  Recall: {history['epoch_recall'][-1]:.4f}\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Copy {onnx_path} to your models/ folder\")\n",
    "print(f\"  2. Update simple_wakeword_test.py to use your model\")\n",
    "print(f\"  3. Test with: python simple_wakeword_test.py\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
